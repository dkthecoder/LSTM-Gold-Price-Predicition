{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.9.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chart-studio in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from chart-studio) (2.22.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from chart-studio) (1.11.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from chart-studio) (4.0.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from chart-studio) (1.3.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->chart-studio) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->chart-studio) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->chart-studio) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->chart-studio) (2019.6.16)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chart-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from plotly) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import pandas\n",
    "import numpy\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1a Load Data //CSV needs to be in coloumbs\n",
    "csv_path = \"/tf/GOLDDaily.csv\"\n",
    "dataOPEN = pandas.read_csv(csv_path, usecols=[1])\n",
    "dataHIGH = pandas.read_csv(csv_path, usecols=[2])\n",
    "dataLOW = pandas.read_csv(csv_path, usecols=[3])\n",
    "dataCLOSE = pandas.read_csv(csv_path, usecols=[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1b Scale data betwen 0 and 1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "openScaled = scaler.fit_transform(dataOPEN)\n",
    "highScaled = scaler.fit_transform(dataHIGH) \n",
    "lowScaled = scaler.fit_transform(dataLOW) \n",
    "closeScaled = scaler.fit_transform(dataCLOSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE!\n",
    "numpy.savetxt('openScaled.csv', openScaled, delimiter=',')\n",
    "numpy.savetxt('highScaled.csv', highScaled, delimiter=',')\n",
    "numpy.savetxt('lowScaled.csv', lowScaled, delimiter=',')\n",
    "numpy.savetxt('close_scaled.csv', closeScaled, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "#allValues for total set of values\n",
    "#splitUnit for values per split\n",
    "splitUnit = 26\n",
    "allValues = 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1c load data into appropriate values, convert to numpy and reshape\n",
    "features_set_open = []  \n",
    "labels_open = []  \n",
    "for i in range(splitUnit, allValues):  \n",
    "    features_set_open.append(openScaled[i-splitUnit:i, 0])\n",
    "    labels_open.append(openScaled[i, 0])\n",
    "features_set_open, labels_open = numpy.array(features_set_open), numpy.array(labels_open)\n",
    "features_set_open = numpy.reshape(features_set_open, (features_set_open.shape[0], features_set_open.shape[1], 1))  \n",
    "\n",
    "features_set_high = []  \n",
    "labels_high = []  \n",
    "for i in range(splitUnit, allValues):  \n",
    "    features_set_high.append(highScaled[i-splitUnit:i, 0])\n",
    "    labels_high.append(highScaled[i, 0])\n",
    "features_set_high, labels_high = numpy.array(features_set_high), numpy.array(labels_high)\n",
    "features_set_high = numpy.reshape(features_set_high, (features_set_high.shape[0], features_set_high.shape[1], 1))  \n",
    "\n",
    "features_set_low = []  \n",
    "labels_low = []  \n",
    "for i in range(splitUnit, allValues):  \n",
    "    features_set_low.append(lowScaled[i-splitUnit:i, 0])\n",
    "    labels_low.append(lowScaled[i, 0])\n",
    "features_set_low, labels_low = numpy.array(features_set_low), numpy.array(labels_low)\n",
    "features_set_low = numpy.reshape(features_set_low, (features_set_low.shape[0], features_set_low.shape[1], 1))  \n",
    "\n",
    "features_set_close = []  \n",
    "labels_close = []  \n",
    "for i in range(splitUnit, allValues):  \n",
    "    features_set_close.append(closeScaled[i-splitUnit:i, 0])\n",
    "    labels_close.append(closeScaled[i, 0])\n",
    "features_set_close, labels_close = numpy.array(features_set_close), numpy.array(labels_close)\n",
    "features_set_close = numpy.reshape(features_set_close, (features_set_close.shape[0], features_set_close.shape[1], 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1d spliting the dataset into training and test set\n",
    "open_x_train, open_x_test, open_y_train, open_y_test = train_test_split(features_set_open, labels_open, test_size=0.05)\n",
    "high_x_train, high_x_test, high_y_train, high_y_test = train_test_split(features_set_high, labels_high, test_size=0.05)\n",
    "low_x_train, low_x_test, low_y_train, low_y_test = train_test_split(features_set_low, labels_low, test_size=0.05)\n",
    "close_x_train, close_x_test, close_y_train, close_y_test = train_test_split(features_set_close, labels_close, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "nodes =10\n",
    "epochUnit = 100\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1686 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "1686/1686 [==============================] - 3s 2ms/step - loss: 0.0532 - val_loss: 0.0043\n",
      "Epoch 2/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 3/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 5/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 6/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 7/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0032 - val_loss: 0.0025\n",
      "Epoch 8/100\n",
      "1686/1686 [==============================] - 2s 986us/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 9/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 10/100\n",
      "1686/1686 [==============================] - 2s 990us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 11/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 12/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 13/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 14/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 15/100\n",
      "1686/1686 [==============================] - 2s 990us/step - loss: 0.0024 - val_loss: 0.0035\n",
      "Epoch 16/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 17/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 18/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 19/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 20/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 21/100\n",
      "1686/1686 [==============================] - 2s 960us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 22/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 23/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 24/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 25/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 26/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "1686/1686 [==============================] - 2s 993us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "1686/1686 [==============================] - 2s 984us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "1686/1686 [==============================] - 2s 1000us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 31/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 32/100\n",
      "1686/1686 [==============================] - 2s 955us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "1686/1686 [==============================] - 2s 995us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 34/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 35/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 36/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 37/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 38/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 39/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 41/100\n",
      "1686/1686 [==============================] - 2s 989us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 42/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 45/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 47/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 48/100\n",
      "1686/1686 [==============================] - 2s 988us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 49/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0012 - val_loss: 9.7252e-04\n",
      "Epoch 50/100\n",
      "1686/1686 [==============================] - 2s 996us/step - loss: 0.0012 - val_loss: 9.7869e-04\n",
      "Epoch 51/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 52/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0013 - val_loss: 8.9514e-04\n",
      "Epoch 53/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0011 - val_loss: 8.8680e-04\n",
      "Epoch 54/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0011 - val_loss: 8.5301e-04\n",
      "Epoch 55/100\n",
      "1686/1686 [==============================] - 2s 979us/step - loss: 0.0011 - val_loss: 9.4494e-04\n",
      "Epoch 56/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0011 - val_loss: 8.5297e-04\n",
      "Epoch 57/100\n",
      "1686/1686 [==============================] - 2s 986us/step - loss: 0.0010 - val_loss: 8.3646e-04\n",
      "Epoch 58/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 0.0011 - val_loss: 8.6001e-04\n",
      "Epoch 59/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 60/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0010 - val_loss: 7.7592e-04\n",
      "Epoch 61/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0010 - val_loss: 7.2865e-04\n",
      "Epoch 62/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 9.6567e-04 - val_loss: 7.5512e-04\n",
      "Epoch 63/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 9.7955e-04 - val_loss: 7.0124e-04\n",
      "Epoch 64/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 9.7798e-04 - val_loss: 9.7885e-04\n",
      "Epoch 65/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 9.9911e-04 - val_loss: 7.5653e-04\n",
      "Epoch 66/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 9.6179e-04 - val_loss: 8.2099e-04\n",
      "Epoch 67/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 9.8560e-04 - val_loss: 7.3134e-04\n",
      "Epoch 68/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0010 - val_loss: 6.4761e-04\n",
      "Epoch 69/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 9.7106e-04 - val_loss: 6.6863e-04\n",
      "Epoch 70/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 9.2687e-04 - val_loss: 6.5577e-04\n",
      "Epoch 71/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 8.9803e-04 - val_loss: 7.4137e-04\n",
      "Epoch 72/100\n",
      "1686/1686 [==============================] - 2s 979us/step - loss: 8.7302e-04 - val_loss: 7.0196e-04\n",
      "Epoch 73/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 9.5863e-04 - val_loss: 6.1127e-04\n",
      "Epoch 74/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 8.6001e-04 - val_loss: 5.8004e-04\n",
      "Epoch 75/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 8.8591e-04 - val_loss: 5.7410e-04\n",
      "Epoch 76/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 8.1748e-04 - val_loss: 5.8711e-04\n",
      "Epoch 77/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 8.9567e-04 - val_loss: 5.4881e-04\n",
      "Epoch 78/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 8.9979e-04 - val_loss: 5.3323e-04\n",
      "Epoch 79/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 7.7211e-04 - val_loss: 5.2562e-04\n",
      "Epoch 80/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 8.6915e-04 - val_loss: 6.1566e-04\n",
      "Epoch 81/100\n",
      "1686/1686 [==============================] - 2s 997us/step - loss: 8.0188e-04 - val_loss: 5.3666e-04\n",
      "Epoch 82/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 7.6424e-04 - val_loss: 4.9542e-04\n",
      "Epoch 83/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 8.1541e-04 - val_loss: 6.7102e-04\n",
      "Epoch 84/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 8.5639e-04 - val_loss: 4.7466e-04\n",
      "Epoch 85/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 8.1739e-04 - val_loss: 7.0205e-04\n",
      "Epoch 86/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 7.6326e-04 - val_loss: 4.6211e-04\n",
      "Epoch 87/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 8.0352e-04 - val_loss: 4.4167e-04\n",
      "Epoch 88/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 7.8268e-04 - val_loss: 4.4326e-04\n",
      "Epoch 89/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 7.2839e-04 - val_loss: 4.9845e-04\n",
      "Epoch 90/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 6.9593e-04 - val_loss: 4.2337e-04\n",
      "Epoch 91/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 7.1060e-04 - val_loss: 4.2127e-04\n",
      "Epoch 92/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 6.8679e-04 - val_loss: 4.0975e-04\n",
      "Epoch 93/100\n",
      "1686/1686 [==============================] - 2s 959us/step - loss: 7.8947e-04 - val_loss: 5.0142e-04\n",
      "Epoch 94/100\n",
      "1686/1686 [==============================] - 2s 996us/step - loss: 8.4357e-04 - val_loss: 6.6299e-04\n",
      "Epoch 95/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 7.5442e-04 - val_loss: 5.0747e-04\n",
      "Epoch 96/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 7.6201e-04 - val_loss: 4.3081e-04\n",
      "Epoch 97/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 6.5356e-04 - val_loss: 3.7725e-04\n",
      "Epoch 98/100\n",
      "1686/1686 [==============================] - 2s 986us/step - loss: 7.4235e-04 - val_loss: 6.5998e-04\n",
      "Epoch 99/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 6.9276e-04 - val_loss: 4.1650e-04\n",
      "Epoch 100/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 6.5727e-04 - val_loss: 4.4013e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e5d32add8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2a Build Model For Open\n",
    "modelOpen = Sequential()\n",
    "modelOpen.add(LSTM(nodes, return_sequences=True, input_shape=(features_set_open.shape[1],1)))  \n",
    "modelOpen.add(Dropout(dropout))\n",
    "\n",
    "modelOpen.add(LSTM(nodes, return_sequences=True))\n",
    "modelOpen.add(Dropout(dropout))\n",
    "\n",
    "modelOpen.add(LSTM(nodes))\n",
    "\n",
    "modelOpen.add(Dense(1))\n",
    "\n",
    "modelOpen.compile(loss='mse', optimizer='adam')\n",
    "modelOpen.fit(open_x_train, open_y_train, batch_size=26, epochs=epochUnit, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1686 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "1686/1686 [==============================] - 3s 2ms/step - loss: 0.0498 - val_loss: 0.0061\n",
      "Epoch 2/100\n",
      "1686/1686 [==============================] - 2s 932us/step - loss: 0.0038 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "1686/1686 [==============================] - 2s 937us/step - loss: 0.0037 - val_loss: 0.0026\n",
      "Epoch 4/100\n",
      "1686/1686 [==============================] - 2s 943us/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 5/100\n",
      "1686/1686 [==============================] - 2s 934us/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 6/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 7/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 8/100\n",
      "1686/1686 [==============================] - 2s 955us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 9/100\n",
      "1686/1686 [==============================] - 2s 938us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 10/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 11/100\n",
      "1686/1686 [==============================] - 2s 937us/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 12/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 13/100\n",
      "1686/1686 [==============================] - 2s 939us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 14/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 15/100\n",
      "1686/1686 [==============================] - 2s 933us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 16/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 17/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 18/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 19/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 20/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 21/100\n",
      "1686/1686 [==============================] - 2s 991us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 22/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 23/100\n",
      "1686/1686 [==============================] - 2s 984us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 24/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 25/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 26/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 27/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 28/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 30/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "1686/1686 [==============================] - 2s 988us/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 34/100\n",
      "1686/1686 [==============================] - 2s 994us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 35/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "1686/1686 [==============================] - 2s 995us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 37/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 38/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 39/100\n",
      "1686/1686 [==============================] - 2s 977us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 40/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 41/100\n",
      "1686/1686 [==============================] - 2s 996us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "1686/1686 [==============================] - 2s 990us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 43/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 44/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 45/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 46/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 47/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 48/100\n",
      "1686/1686 [==============================] - 2s 991us/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 49/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 50/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 51/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0013 - val_loss: 9.5758e-04\n",
      "Epoch 52/100\n",
      "1686/1686 [==============================] - 2s 984us/step - loss: 0.0012 - val_loss: 9.4363e-04\n",
      "Epoch 53/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 0.0012 - val_loss: 9.4824e-04\n",
      "Epoch 54/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 55/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0012 - val_loss: 8.7869e-04\n",
      "Epoch 56/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 0.0011 - val_loss: 8.7900e-04\n",
      "Epoch 57/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0011 - val_loss: 9.5328e-04\n",
      "Epoch 58/100\n",
      "1686/1686 [==============================] - 2s 977us/step - loss: 0.0010 - val_loss: 9.0568e-04\n",
      "Epoch 59/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0011 - val_loss: 8.7808e-04\n",
      "Epoch 60/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0010 - val_loss: 7.8515e-04\n",
      "Epoch 61/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0010 - val_loss: 8.0497e-04\n",
      "Epoch 62/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 0.0011 - val_loss: 9.3945e-04\n",
      "Epoch 63/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 64/100\n",
      "1686/1686 [==============================] - 2s 979us/step - loss: 0.0010 - val_loss: 8.3635e-04\n",
      "Epoch 65/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0011 - val_loss: 8.3804e-04\n",
      "Epoch 66/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 9.9355e-04 - val_loss: 7.5107e-04\n",
      "Epoch 67/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.6428e-04 - val_loss: 7.5922e-04\n",
      "Epoch 68/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.6078e-04 - val_loss: 6.6129e-04\n",
      "Epoch 69/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 9.6736e-04 - val_loss: 6.4783e-04\n",
      "Epoch 70/100\n",
      "1686/1686 [==============================] - 2s 994us/step - loss: 9.4333e-04 - val_loss: 6.4881e-04\n",
      "Epoch 71/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 9.2639e-04 - val_loss: 7.6493e-04\n",
      "Epoch 72/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 9.0721e-04 - val_loss: 7.2536e-04\n",
      "Epoch 73/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.3148e-04 - val_loss: 6.3615e-04\n",
      "Epoch 74/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 8.6036e-04 - val_loss: 6.1200e-04\n",
      "Epoch 75/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.1120e-04 - val_loss: 5.9135e-04\n",
      "Epoch 76/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.0089e-04 - val_loss: 8.8925e-04\n",
      "Epoch 77/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 9.5779e-04 - val_loss: 7.3165e-04\n",
      "Epoch 78/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 8.6106e-04 - val_loss: 5.5086e-04\n",
      "Epoch 79/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 8.3742e-04 - val_loss: 5.4633e-04\n",
      "Epoch 80/100\n",
      "1686/1686 [==============================] - 2s 995us/step - loss: 8.0588e-04 - val_loss: 5.6041e-04\n",
      "Epoch 81/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 7.7365e-04 - val_loss: 5.5063e-04\n",
      "Epoch 82/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 7.4909e-04 - val_loss: 5.0600e-04\n",
      "Epoch 83/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 8.1961e-04 - val_loss: 4.8282e-04\n",
      "Epoch 84/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 7.6538e-04 - val_loss: 5.9538e-04\n",
      "Epoch 85/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 7.8339e-04 - val_loss: 5.2086e-04\n",
      "Epoch 86/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 7.7633e-04 - val_loss: 5.4760e-04\n",
      "Epoch 87/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 7.2900e-04 - val_loss: 4.8211e-04\n",
      "Epoch 88/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 7.4632e-04 - val_loss: 4.7482e-04\n",
      "Epoch 89/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 7.2956e-04 - val_loss: 5.2656e-04\n",
      "Epoch 90/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 7.0433e-04 - val_loss: 4.2689e-04\n",
      "Epoch 91/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 7.5497e-04 - val_loss: 5.0402e-04\n",
      "Epoch 92/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 6.7056e-04 - val_loss: 5.5372e-04\n",
      "Epoch 93/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 6.8474e-04 - val_loss: 5.3320e-04\n",
      "Epoch 94/100\n",
      "1686/1686 [==============================] - 2s 1ms/step - loss: 6.9903e-04 - val_loss: 4.7894e-04\n",
      "Epoch 95/100\n",
      "1686/1686 [==============================] - 2s 977us/step - loss: 7.3562e-04 - val_loss: 3.8846e-04\n",
      "Epoch 96/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 6.7983e-04 - val_loss: 4.0052e-04\n",
      "Epoch 97/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 6.2590e-04 - val_loss: 3.9976e-04\n",
      "Epoch 98/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 6.6295e-04 - val_loss: 3.9576e-04\n",
      "Epoch 99/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 5.9736e-04 - val_loss: 3.6426e-04\n",
      "Epoch 100/100\n",
      "1686/1686 [==============================] - 2s 992us/step - loss: 6.8505e-04 - val_loss: 4.8581e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e3ee850f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2b Build Model For High\n",
    "modelHigh = Sequential()\n",
    "modelHigh.add(LSTM(nodes, return_sequences=True, input_shape=(features_set_high.shape[1], 1)))\n",
    "modelHigh.add(Dropout(dropout))\n",
    "\n",
    "modelHigh.add(LSTM(nodes, return_sequences=True))\n",
    "modelHigh.add(Dropout(dropout))\n",
    "\n",
    "modelHigh.add(LSTM(nodes))\n",
    "\n",
    "modelHigh.add(Dense(1))\n",
    "\n",
    "modelHigh.compile(loss='mse', optimizer='adam')\n",
    "modelHigh.fit(high_x_train, high_y_train, batch_size=26, epochs=epochUnit, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1686 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "1686/1686 [==============================] - 3s 2ms/step - loss: 0.0455 - val_loss: 0.0046\n",
      "Epoch 2/100\n",
      "1686/1686 [==============================] - 2s 919us/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 3/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 4/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 5/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 6/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 7/100\n",
      "1686/1686 [==============================] - 2s 942us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 8/100\n",
      "1686/1686 [==============================] - 2s 943us/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 9/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 10/100\n",
      "1686/1686 [==============================] - 2s 946us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 11/100\n",
      "1686/1686 [==============================] - 2s 946us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 12/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 13/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 14/100\n",
      "1686/1686 [==============================] - 2s 949us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 15/100\n",
      "1686/1686 [==============================] - 2s 946us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 16/100\n",
      "1686/1686 [==============================] - 2s 929us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 17/100\n",
      "1686/1686 [==============================] - 2s 952us/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 18/100\n",
      "1686/1686 [==============================] - 2s 938us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 19/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 20/100\n",
      "1686/1686 [==============================] - 2s 946us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 21/100\n",
      "1686/1686 [==============================] - 2s 977us/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 22/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 23/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 24/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 25/100\n",
      "1686/1686 [==============================] - 2s 959us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 26/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 27/100\n",
      "1686/1686 [==============================] - 2s 945us/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 28/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 29/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 30/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 31/100\n",
      "1686/1686 [==============================] - 2s 949us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 32/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 33/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 34/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 35/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 36/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 37/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 38/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 39/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 40/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 41/100\n",
      "1686/1686 [==============================] - 2s 955us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 42/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 43/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 45/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 46/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 48/100\n",
      "1686/1686 [==============================] - 2s 952us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 49/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 50/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 51/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 54/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 55/100\n",
      "1686/1686 [==============================] - 2s 959us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 56/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 59/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 60/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 61/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 62/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 63/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 9.9773e-04 - val_loss: 0.0013\n",
      "Epoch 64/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 65/100\n",
      "1686/1686 [==============================] - 2s 999us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 66/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 9.8145e-04 - val_loss: 0.0013\n",
      "Epoch 67/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 9.5119e-04 - val_loss: 0.0013\n",
      "Epoch 68/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 9.3984e-04 - val_loss: 0.0014\n",
      "Epoch 69/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 9.5285e-04 - val_loss: 0.0013\n",
      "Epoch 70/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 9.3917e-04 - val_loss: 0.0014\n",
      "Epoch 71/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 9.6739e-04 - val_loss: 0.0013\n",
      "Epoch 72/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 9.5166e-04 - val_loss: 0.0013\n",
      "Epoch 73/100\n",
      "1686/1686 [==============================] - 2s 949us/step - loss: 9.7328e-04 - val_loss: 0.0013\n",
      "Epoch 74/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 9.1021e-04 - val_loss: 0.0011\n",
      "Epoch 75/100\n",
      "1686/1686 [==============================] - 2s 960us/step - loss: 9.4789e-04 - val_loss: 0.0012\n",
      "Epoch 76/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 9.1475e-04 - val_loss: 0.0012\n",
      "Epoch 77/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 9.5475e-04 - val_loss: 0.0012\n",
      "Epoch 78/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 9.5732e-04 - val_loss: 0.0011\n",
      "Epoch 79/100\n",
      "1686/1686 [==============================] - 2s 960us/step - loss: 8.7777e-04 - val_loss: 0.0012\n",
      "Epoch 80/100\n",
      "1686/1686 [==============================] - 2s 955us/step - loss: 8.5140e-04 - val_loss: 0.0011\n",
      "Epoch 81/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 8.1763e-04 - val_loss: 0.0012\n",
      "Epoch 82/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 8.4690e-04 - val_loss: 0.0011\n",
      "Epoch 83/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 8.2385e-04 - val_loss: 0.0010\n",
      "Epoch 84/100\n",
      "1686/1686 [==============================] - 2s 940us/step - loss: 8.3535e-04 - val_loss: 0.0012\n",
      "Epoch 85/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 7.7799e-04 - val_loss: 0.0010\n",
      "Epoch 86/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 8.0599e-04 - val_loss: 0.0010\n",
      "Epoch 87/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 7.9525e-04 - val_loss: 0.0011\n",
      "Epoch 88/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 7.8811e-04 - val_loss: 0.0010\n",
      "Epoch 89/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 8.5632e-04 - val_loss: 9.9327e-04\n",
      "Epoch 90/100\n",
      "1686/1686 [==============================] - 2s 952us/step - loss: 6.8799e-04 - val_loss: 9.4156e-04\n",
      "Epoch 91/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 7.6833e-04 - val_loss: 0.0011\n",
      "Epoch 92/100\n",
      "1686/1686 [==============================] - 2s 945us/step - loss: 8.1985e-04 - val_loss: 0.0011\n",
      "Epoch 93/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 7.2894e-04 - val_loss: 8.3825e-04\n",
      "Epoch 94/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 7.1070e-04 - val_loss: 9.9305e-04\n",
      "Epoch 95/100\n",
      "1686/1686 [==============================] - 2s 977us/step - loss: 7.2461e-04 - val_loss: 9.8487e-04\n",
      "Epoch 96/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 8.0607e-04 - val_loss: 0.0011\n",
      "Epoch 97/100\n",
      "1686/1686 [==============================] - 2s 979us/step - loss: 6.8086e-04 - val_loss: 8.8025e-04\n",
      "Epoch 98/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 7.0031e-04 - val_loss: 8.7090e-04\n",
      "Epoch 99/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 7.6963e-04 - val_loss: 8.6616e-04\n",
      "Epoch 100/100\n",
      "1686/1686 [==============================] - 2s 966us/step - loss: 6.7610e-04 - val_loss: 8.0088e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e3cfeaa20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2c Build Model For Low\n",
    "modelLow = Sequential()\n",
    "modelLow.add(LSTM(nodes, return_sequences=True, input_shape=(features_set_low.shape[1], 1)))\n",
    "modelLow.add(Dropout(dropout))\n",
    "\n",
    "modelLow.add(LSTM(nodes, return_sequences=True))\n",
    "modelLow.add(Dropout(dropout))\n",
    "\n",
    "modelLow.add(LSTM(nodes))\n",
    "\n",
    "modelLow.add(Dense(1))\n",
    "\n",
    "modelLow.compile(loss='mse', optimizer='adam')\n",
    "modelLow.fit(low_x_train, low_y_train, batch_size=26, epochs=epochUnit, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1686 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "1686/1686 [==============================] - 4s 2ms/step - loss: 0.0633 - val_loss: 0.0073\n",
      "Epoch 2/100\n",
      "1686/1686 [==============================] - 2s 932us/step - loss: 0.0053 - val_loss: 0.0022\n",
      "Epoch 3/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 4/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 5/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 6/100\n",
      "1686/1686 [==============================] - 2s 946us/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 9/100\n",
      "1686/1686 [==============================] - 2s 943us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 10/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 11/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 12/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 13/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 14/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 15/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 16/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "1686/1686 [==============================] - 2s 952us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 18/100\n",
      "1686/1686 [==============================] - 2s 987us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 19/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 20/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 21/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 23/100\n",
      "1686/1686 [==============================] - 2s 958us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 26/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 27/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 28/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 29/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "1686/1686 [==============================] - 2s 985us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 31/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "1686/1686 [==============================] - 2s 960us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 33/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "1686/1686 [==============================] - 2s 976us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 38/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0014 - val_loss: 9.6780e-04\n",
      "Epoch 39/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0014 - val_loss: 9.6594e-04\n",
      "Epoch 40/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 41/100\n",
      "1686/1686 [==============================] - 2s 982us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "1686/1686 [==============================] - 2s 964us/step - loss: 0.0013 - val_loss: 9.1985e-04\n",
      "Epoch 43/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 0.0012 - val_loss: 8.9468e-04\n",
      "Epoch 44/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 0.0013 - val_loss: 9.0075e-04\n",
      "Epoch 45/100\n",
      "1686/1686 [==============================] - 2s 974us/step - loss: 0.0013 - val_loss: 8.9478e-04\n",
      "Epoch 46/100\n",
      "1686/1686 [==============================] - 2s 943us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 0.0012 - val_loss: 8.8914e-04\n",
      "Epoch 48/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 0.0012 - val_loss: 9.7060e-04\n",
      "Epoch 49/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 0.0012 - val_loss: 8.5694e-04\n",
      "Epoch 50/100\n",
      "1686/1686 [==============================] - 2s 969us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 51/100\n",
      "1686/1686 [==============================] - 2s 971us/step - loss: 0.0012 - val_loss: 9.5365e-04\n",
      "Epoch 52/100\n",
      "1686/1686 [==============================] - 2s 949us/step - loss: 0.0012 - val_loss: 8.8560e-04\n",
      "Epoch 53/100\n",
      "1686/1686 [==============================] - 2s 981us/step - loss: 0.0011 - val_loss: 8.0846e-04\n",
      "Epoch 54/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0010 - val_loss: 7.9772e-04\n",
      "Epoch 55/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 0.0010 - val_loss: 7.3397e-04\n",
      "Epoch 56/100\n",
      "1686/1686 [==============================] - 2s 960us/step - loss: 0.0010 - val_loss: 7.0950e-04\n",
      "Epoch 57/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 0.0011 - val_loss: 7.4897e-04\n",
      "Epoch 58/100\n",
      "1686/1686 [==============================] - 2s 954us/step - loss: 9.6766e-04 - val_loss: 7.5683e-04\n",
      "Epoch 59/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 9.9423e-04 - val_loss: 6.6285e-04\n",
      "Epoch 60/100\n",
      "1686/1686 [==============================] - 2s 980us/step - loss: 9.9944e-04 - val_loss: 9.2201e-04\n",
      "Epoch 61/100\n",
      "1686/1686 [==============================] - 2s 948us/step - loss: 0.0010 - val_loss: 6.8672e-04\n",
      "Epoch 62/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 9.2122e-04 - val_loss: 8.6331e-04\n",
      "Epoch 63/100\n",
      "1686/1686 [==============================] - 2s 944us/step - loss: 9.6183e-04 - val_loss: 6.8703e-04\n",
      "Epoch 64/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 8.7767e-04 - val_loss: 6.4361e-04\n",
      "Epoch 65/100\n",
      "1686/1686 [==============================] - 2s 967us/step - loss: 8.5470e-04 - val_loss: 6.4274e-04\n",
      "Epoch 66/100\n",
      "1686/1686 [==============================] - 2s 965us/step - loss: 8.5934e-04 - val_loss: 6.0687e-04\n",
      "Epoch 67/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 8.9031e-04 - val_loss: 5.6456e-04\n",
      "Epoch 68/100\n",
      "1686/1686 [==============================] - 2s 988us/step - loss: 8.3851e-04 - val_loss: 5.9079e-04\n",
      "Epoch 69/100\n",
      "1686/1686 [==============================] - 2s 973us/step - loss: 8.0751e-04 - val_loss: 6.6830e-04\n",
      "Epoch 70/100\n",
      "1686/1686 [==============================] - 2s 970us/step - loss: 8.7459e-04 - val_loss: 5.3035e-04\n",
      "Epoch 71/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 8.6195e-04 - val_loss: 6.6230e-04\n",
      "Epoch 72/100\n",
      "1686/1686 [==============================] - 2s 944us/step - loss: 7.8580e-04 - val_loss: 5.0846e-04\n",
      "Epoch 73/100\n",
      "1686/1686 [==============================] - 2s 984us/step - loss: 7.2106e-04 - val_loss: 6.1705e-04\n",
      "Epoch 74/100\n",
      "1686/1686 [==============================] - 2s 963us/step - loss: 8.5011e-04 - val_loss: 4.8460e-04\n",
      "Epoch 75/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 8.2909e-04 - val_loss: 5.0132e-04\n",
      "Epoch 76/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 6.9868e-04 - val_loss: 4.9197e-04\n",
      "Epoch 77/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 7.8898e-04 - val_loss: 5.6093e-04\n",
      "Epoch 78/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 7.8741e-04 - val_loss: 5.3992e-04\n",
      "Epoch 79/100\n",
      "1686/1686 [==============================] - 2s 979us/step - loss: 7.6527e-04 - val_loss: 4.3184e-04\n",
      "Epoch 80/100\n",
      "1686/1686 [==============================] - 2s 959us/step - loss: 6.7804e-04 - val_loss: 4.8191e-04\n",
      "Epoch 81/100\n",
      "1686/1686 [==============================] - 2s 968us/step - loss: 7.0670e-04 - val_loss: 5.5783e-04\n",
      "Epoch 82/100\n",
      "1686/1686 [==============================] - 2s 978us/step - loss: 7.2965e-04 - val_loss: 5.3453e-04\n",
      "Epoch 83/100\n",
      "1686/1686 [==============================] - 2s 961us/step - loss: 7.5105e-04 - val_loss: 6.4379e-04\n",
      "Epoch 84/100\n",
      "1686/1686 [==============================] - 2s 953us/step - loss: 6.9359e-04 - val_loss: 4.5950e-04\n",
      "Epoch 85/100\n",
      "1686/1686 [==============================] - 2s 956us/step - loss: 7.4452e-04 - val_loss: 4.5896e-04\n",
      "Epoch 86/100\n",
      "1686/1686 [==============================] - 2s 996us/step - loss: 6.8139e-04 - val_loss: 3.9696e-04\n",
      "Epoch 87/100\n",
      "1686/1686 [==============================] - 2s 952us/step - loss: 6.8990e-04 - val_loss: 3.8446e-04\n",
      "Epoch 88/100\n",
      "1686/1686 [==============================] - 2s 975us/step - loss: 6.7707e-04 - val_loss: 5.3077e-04\n",
      "Epoch 89/100\n",
      "1686/1686 [==============================] - 2s 951us/step - loss: 6.7506e-04 - val_loss: 5.1997e-04\n",
      "Epoch 90/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 6.6194e-04 - val_loss: 4.3033e-04\n",
      "Epoch 91/100\n",
      "1686/1686 [==============================] - 2s 972us/step - loss: 6.2057e-04 - val_loss: 3.7711e-04\n",
      "Epoch 92/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 6.9466e-04 - val_loss: 3.6897e-04\n",
      "Epoch 93/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 6.2382e-04 - val_loss: 3.9621e-04\n",
      "Epoch 94/100\n",
      "1686/1686 [==============================] - 2s 939us/step - loss: 6.7063e-04 - val_loss: 3.5896e-04\n",
      "Epoch 95/100\n",
      "1686/1686 [==============================] - 2s 992us/step - loss: 6.4941e-04 - val_loss: 3.9257e-04\n",
      "Epoch 96/100\n",
      "1686/1686 [==============================] - 2s 957us/step - loss: 6.6138e-04 - val_loss: 3.7057e-04\n",
      "Epoch 97/100\n",
      "1686/1686 [==============================] - 2s 962us/step - loss: 6.5277e-04 - val_loss: 3.8172e-04\n",
      "Epoch 98/100\n",
      "1686/1686 [==============================] - 2s 947us/step - loss: 6.1811e-04 - val_loss: 5.4016e-04\n",
      "Epoch 99/100\n",
      "1686/1686 [==============================] - 2s 983us/step - loss: 6.4724e-04 - val_loss: 3.6180e-04\n",
      "Epoch 100/100\n",
      "1686/1686 [==============================] - 2s 949us/step - loss: 6.2987e-04 - val_loss: 3.9940e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e36a9ae10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2d Build Model For Close\n",
    "modelClose = Sequential()\n",
    "modelClose.add(LSTM(nodes, return_sequences=True, input_shape=(features_set_close.shape[1], 1)))\n",
    "modelClose.add(Dropout(dropout))\n",
    "\n",
    "modelClose.add(LSTM(nodes, return_sequences=True))\n",
    "modelClose.add(Dropout(dropout))\n",
    "\n",
    "modelClose.add(LSTM(nodes))\n",
    "\n",
    "modelClose.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "modelClose.compile(loss='mse', optimizer='adam')\n",
    "modelClose.fit(close_x_train, close_y_train, batch_size=26, epochs=epochUnit, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3a - Predict with test data!\n",
    "predictions_open = modelOpen.predict(open_x_test)\n",
    "predictions_high = modelHigh.predict(high_x_test)\n",
    "predictions_low = modelLow.predict(low_x_test)\n",
    "predictions_close = modelClose.predict(close_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE!\n",
    "numpy.savetxt('predictions_open.csv', predictions_open, delimiter=',')\n",
    "numpy.savetxt('predictions_high.csv', predictions_high, delimiter=',')\n",
    "numpy.savetxt('predictions_low.csv', predictions_low, delimiter=',')\n",
    "numpy.savetxt('predictions_close.csv', predictions_close, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3b - Inverse scale!\n",
    "inversed_open = scaler.inverse_transform(predictions_open)\n",
    "inversed_high = scaler.inverse_transform(predictions_high) \n",
    "inversed_low = scaler.inverse_transform(predictions_low) \n",
    "inversed_close = scaler.inverse_transform(predictions_close) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE!\n",
    "numpy.savetxt('inversed_open.csv', inversed_open, delimiter=',')\n",
    "numpy.savetxt('inversed_high.csv', inversed_high, delimiter=',')\n",
    "numpy.savetxt('inversed_low.csv', inversed_low, delimiter=',')\n",
    "numpy.savetxt('inversed_close.csv', inversed_close, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2-D, but have shapes (99,) and (99, 26, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-93c9dda8ec56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_x_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m     return gca().plot(\n\u001b[1;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2789\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n\u001b[0;32m--> 273\u001b[0;31m                              \"shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y can be no greater than 2-D, but have shapes (99,) and (99, 26, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(close_x_test)\n",
    "plt.plot(predictions_close)\n",
    "plt.legend(['real', 'predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
